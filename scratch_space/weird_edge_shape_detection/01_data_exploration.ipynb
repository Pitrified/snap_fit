{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbbad11",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4607b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ffed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from loguru import logger as lg\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich import get_console\n",
    "\n",
    "# Rich Jupyter fix\n",
    "console: Console = get_console()\n",
    "console.is_jupyter = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snap_fit.config.aruco.aruco_board_config import ArucoBoardConfig\n",
    "from snap_fit.config.aruco.aruco_detector_config import ArucoDetectorConfig\n",
    "from snap_fit.params.snap_fit_params import get_snap_fit_paths\n",
    "from snap_fit.puzzle.sheet_aruco import SheetAruco\n",
    "from snap_fit.puzzle.sheet_manager import SheetManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9524597d",
   "metadata": {},
   "source": [
    "## Helper: Load Puzzle Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_puzzle_sheets(puzzle_name: str) -> SheetManager:\n",
    "    \"\"\"Load puzzle sheets for a given puzzle name (e.g., 'sample_puzzle_v1').\"\"\"\n",
    "    # Configure ArUco detection\n",
    "    board_config = ArucoBoardConfig(\n",
    "        markers_x=7,\n",
    "        markers_y=5,\n",
    "        marker_length=100,\n",
    "        marker_separation=100,\n",
    "    )\n",
    "    detector_config = ArucoDetectorConfig(board=board_config)\n",
    "\n",
    "    # Initialize sheet loader\n",
    "    sheet_aruco = SheetAruco(detector_config)\n",
    "    aruco_loader = partial(sheet_aruco.load_sheet, min_area=5_000)\n",
    "\n",
    "    # Load puzzle sheets\n",
    "    paths = get_snap_fit_paths()\n",
    "    data_dir = paths.data_fol / puzzle_name / \"sheets\"\n",
    "    lg.info(f\"Loading data from {data_dir}\")\n",
    "\n",
    "    manager = SheetManager()\n",
    "    manager.add_sheets(\n",
    "        folder_path=data_dir,\n",
    "        pattern=\"*.png\",\n",
    "        loader_func=aruco_loader,\n",
    "    )\n",
    "\n",
    "    lg.info(\n",
    "        f\"Loaded {len(manager.get_sheets_ls())} sheets with \"\n",
    "        f\"{len(manager.get_pieces_ls())} pieces\"\n",
    "    )\n",
    "    return manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a10c00",
   "metadata": {},
   "source": [
    "## Load v1 and v2 Puzzle Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_v1 = load_puzzle_sheets(\"sample_puzzle_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_v2 = load_puzzle_sheets(\"sample_puzzle_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute expected segment counts for a 6x8 puzzle\n",
    "ROWS, COLS = 6, 8\n",
    "\n",
    "total_pieces = ROWS * COLS  # 48\n",
    "total_segments = total_pieces * 4  # 192\n",
    "\n",
    "# EDGE segments are on the puzzle boundary (perimeter)\n",
    "perimeter_segments = 2 * (ROWS + COLS)  # 28\n",
    "\n",
    "# Internal segments should be IN or OUT (each internal edge has one of each)\n",
    "internal_horizontal_edges = ROWS * (COLS - 1)  # 42\n",
    "internal_vertical_edges = (ROWS - 1) * COLS  # 40\n",
    "total_internal_edges = internal_horizontal_edges + internal_vertical_edges  # 82\n",
    "\n",
    "# Each internal edge contributes one IN and one OUT segment\n",
    "expected_in = total_internal_edges  # 82\n",
    "expected_out = total_internal_edges  # 82\n",
    "expected_edge = perimeter_segments  # 28\n",
    "expected_weird = 0  # ideally none!\n",
    "\n",
    "rprint(\"[bold]Expected Segment Distribution for 6x8 Puzzle[/bold]\")\n",
    "rprint(f\"  Total segments: {total_segments}\")\n",
    "rprint(\n",
    "    f\"  EDGE (boundary): {expected_edge} ({100 * expected_edge / total_segments:.1f}%)\"\n",
    ")\n",
    "rprint(f\"  IN:   {expected_in} ({100 * expected_in / total_segments:.1f}%)\")\n",
    "rprint(f\"  OUT:  {expected_out} ({100 * expected_out / total_segments:.1f}%)\")\n",
    "rprint(f\"  WEIRD (target): {expected_weird} (0.0%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329023c4",
   "metadata": {},
   "source": [
    "## Analyze Shape Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_shapes(manager: SheetManager, label: str) -> Counter:\n",
    "    \"\"\"Count segment shapes across all pieces.\"\"\"\n",
    "    shapes = []\n",
    "    for piece in manager.get_pieces_ls():\n",
    "        for seg in piece.segments.values():\n",
    "            shapes.append(seg.shape)\n",
    "\n",
    "    counts = Counter(shapes)\n",
    "    total = sum(counts.values())\n",
    "\n",
    "    rprint(f\"\\n[bold]{label}[/bold]\")\n",
    "    rprint(f\"  Total segments: {total}\")\n",
    "    for shape, count in counts.most_common():\n",
    "        pct = 100 * count / total\n",
    "        rprint(f\"  {shape.name:>6}: {count:>3} ({pct:5.1f}%)\")\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0559f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_v1 = analyze_shapes(manager_v1, \"sample_puzzle_v1\")\n",
    "counts_v2 = analyze_shapes(manager_v2, \"sample_puzzle_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234d1c8",
   "metadata": {},
   "source": [
    "## Visualize WEIRD Segments\n",
    "\n",
    "Let's look at segments classified as WEIRD to understand the failure mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ff32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snap_fit.image.segment import SegmentShape\n",
    "\n",
    "\n",
    "def get_weird_segments(manager: SheetManager, max_count: int = 10):\n",
    "    \"\"\"Get a sample of WEIRD segments for visualization.\"\"\"\n",
    "    weird_segments = []\n",
    "    for piece in manager.get_pieces_ls():\n",
    "        for seg_id, seg in piece.segments.items():\n",
    "            if seg.shape == SegmentShape.WEIRD:\n",
    "                weird_segments.append((piece.piece_id, seg_id, seg))\n",
    "                if len(weird_segments) >= max_count:\n",
    "                    return weird_segments\n",
    "    return weird_segments\n",
    "\n",
    "\n",
    "weird_v1 = get_weird_segments(manager_v1, max_count=6)\n",
    "rprint(f\"Found {len(weird_v1)} WEIRD segments to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segment_points(seg, ax, title: str):\n",
    "    \"\"\"Plot segment points showing the classification problem.\"\"\"\n",
    "    # seg.points has shape (N, 1, 2) - OpenCV contour format\n",
    "    pts = seg.points.squeeze()  # Now (N, 2)\n",
    "\n",
    "    # Plot raw points\n",
    "    ax.plot(pts[:, 0], pts[:, 1], \"b-\", linewidth=2, label=\"Segment\")\n",
    "    ax.scatter(pts[0, 0], pts[0, 1], c=\"green\", s=100, zorder=5, label=\"Start\")\n",
    "    ax.scatter(pts[-1, 0], pts[-1, 1], c=\"red\", s=100, zorder=5, label=\"End\")\n",
    "\n",
    "    ax.set_title(f\"{title}\\nShape: {seg.shape.name}\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize WEIRD segments\n",
    "if weird_v1:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (piece_id, seg_id, seg) in enumerate(weird_v1[:6]):\n",
    "        plot_segment_points(seg, axes[i], f\"Piece {piece_id}, Seg {seg_id}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd984644",
   "metadata": {},
   "source": [
    "## Analyze Point Distribution (Understanding the Classification)\n",
    "\n",
    "The current algorithm counts points beyond `flat_th=20` on each side of the center line.\n",
    "Let's see the actual distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41979ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_segment_distribution(seg):\n",
    "    \"\"\"Analyze segment point distribution relative to center line.\n",
    "\n",
    "    Returns dict with statistics about point distribution.\n",
    "    \"\"\"\n",
    "    # seg.points has shape (N, 1, 2) - OpenCV contour format\n",
    "    pts = seg.points.squeeze()  # Now (N, 2)\n",
    "\n",
    "    # Transform to align start-end with x-axis (mimicking _compute_shape)\n",
    "    start, end = pts[0], pts[-1]\n",
    "    direction = end - start\n",
    "    length = np.linalg.norm(direction)\n",
    "\n",
    "    if length < 1e-6:\n",
    "        return None\n",
    "\n",
    "    # Rotation to align with x-axis\n",
    "    angle = np.arctan2(direction[1], direction[0])\n",
    "    cos_a, sin_a = np.cos(-angle), np.sin(-angle)\n",
    "\n",
    "    # Transform points\n",
    "    centered = pts - start\n",
    "    rotated = np.column_stack(\n",
    "        [\n",
    "            centered[:, 0] * cos_a - centered[:, 1] * sin_a,\n",
    "            centered[:, 0] * sin_a + centered[:, 1] * cos_a,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Y values are perpendicular distances from center line\n",
    "    y_vals = rotated[:, 1]\n",
    "\n",
    "    # Current thresholds\n",
    "    flat_th = 20\n",
    "    count_th = 5\n",
    "\n",
    "    out_count = (y_vals < -flat_th).sum()  # Points below line\n",
    "    in_count = (y_vals > flat_th).sum()  # Points above line\n",
    "\n",
    "    return {\n",
    "        \"shape\": seg.shape,\n",
    "        \"y_vals\": y_vals,\n",
    "        \"mean_y\": np.mean(y_vals),\n",
    "        \"std_y\": np.std(y_vals),\n",
    "        \"min_y\": np.min(y_vals),\n",
    "        \"max_y\": np.max(y_vals),\n",
    "        \"out_count\": out_count,\n",
    "        \"in_count\": in_count,\n",
    "        \"is_weird\": out_count > count_th and in_count > count_th,\n",
    "        \"signed_area\": np.trapz(y_vals),\n",
    "        \"length\": length,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all segments\n",
    "all_stats_v1 = []\n",
    "for piece in manager_v1.get_pieces_ls():\n",
    "    for seg_id, seg in piece.segments.items():\n",
    "        stats = analyze_segment_distribution(seg)\n",
    "        if stats:\n",
    "            stats[\"piece_id\"] = piece.piece_id\n",
    "            stats[\"seg_id\"] = seg_id\n",
    "            all_stats_v1.append(stats)\n",
    "\n",
    "rprint(f\"Analyzed {len(all_stats_v1)} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions by shape type\n",
    "from snap_fit.image.segment import SegmentShape\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "shape_types = [SegmentShape.IN, SegmentShape.OUT, SegmentShape.EDGE, SegmentShape.WEIRD]\n",
    "colors = [\"blue\", \"green\", \"orange\", \"red\"]\n",
    "\n",
    "for ax, shape, color in zip(axes.flatten(), shape_types, colors):\n",
    "    shape_stats = [s for s in all_stats_v1 if s[\"shape\"] == shape]\n",
    "\n",
    "    if shape_stats:\n",
    "        mean_ys = [s[\"mean_y\"] for s in shape_stats]\n",
    "        signed_areas = [s[\"signed_area\"] for s in shape_stats]\n",
    "\n",
    "        ax.scatter(mean_ys, signed_areas, c=color, alpha=0.6, s=50)\n",
    "        ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "        ax.axvline(x=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    ax.set_title(f\"{shape.name} (n={len(shape_stats)})\")\n",
    "    ax.set_xlabel(\"Mean Y (perpendicular displacement)\")\n",
    "    ax.set_ylabel(\"Signed Area\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9b427e",
   "metadata": {},
   "source": [
    "## Compare Classification Approaches\n",
    "\n",
    "Let's test alternative classification methods on the existing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ca53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_by_mean(stats, threshold=10):\n",
    "    \"\"\"Option B: Classify by mean displacement.\"\"\"\n",
    "    mean_y = stats[\"mean_y\"]\n",
    "    if mean_y < -threshold:\n",
    "        return \"OUT\"\n",
    "    elif mean_y > threshold:\n",
    "        return \"IN\"\n",
    "    else:\n",
    "        return \"EDGE\"\n",
    "\n",
    "\n",
    "def classify_by_area(stats, threshold=500):\n",
    "    \"\"\"Option B: Classify by signed area.\"\"\"\n",
    "    area = stats[\"signed_area\"]\n",
    "    if area < -threshold:\n",
    "        return \"OUT\"\n",
    "    elif area > threshold:\n",
    "        return \"IN\"\n",
    "    else:\n",
    "        return \"EDGE\"\n",
    "\n",
    "\n",
    "def classify_adaptive(stats):\n",
    "    \"\"\"Option A: Adaptive thresholds based on segment stats.\"\"\"\n",
    "    y_vals = stats[\"y_vals\"]\n",
    "    flat_th = max(10, np.std(y_vals) * 1.5)\n",
    "    count_th = max(3, len(y_vals) * 0.05)\n",
    "\n",
    "    out_count = (y_vals < -flat_th).sum()\n",
    "    in_count = (y_vals > flat_th).sum()\n",
    "\n",
    "    # Convert numpy bools to Python bools for pattern matching\n",
    "    is_out = bool(out_count > count_th)\n",
    "    is_in = bool(in_count > count_th)\n",
    "\n",
    "    match (is_out, is_in):\n",
    "        case (True, False):\n",
    "            return \"OUT\"\n",
    "        case (False, True):\n",
    "            return \"IN\"\n",
    "        case (False, False):\n",
    "            return \"EDGE\"\n",
    "        case (True, True):\n",
    "            return \"WEIRD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare classification methods\n",
    "methods = {\n",
    "    \"Current\": lambda s: s[\"shape\"].name,\n",
    "    \"Mean (th=10)\": lambda s: classify_by_mean(s, threshold=10),\n",
    "    \"Mean (th=15)\": lambda s: classify_by_mean(s, threshold=15),\n",
    "    \"Area (th=50)\": lambda s: classify_by_area(s, threshold=50),\n",
    "    \"Area (th=500)\": lambda s: classify_by_area(s, threshold=500),\n",
    "    \"Area (th=1000)\": lambda s: classify_by_area(s, threshold=1000),\n",
    "    \"Adaptive\": classify_adaptive,\n",
    "}\n",
    "\n",
    "rprint(\"\\n[bold]Classification Method Comparison (v1)[/bold]\")\n",
    "if not all_stats_v1:\n",
    "    rprint(\"[red]No stats available - rerun previous cells[/red]\")\n",
    "else:\n",
    "    for method_name, classify_fn in methods.items():\n",
    "        classifications = [classify_fn(s) for s in all_stats_v1]\n",
    "        counts = Counter(classifications)\n",
    "        total = len(classifications)\n",
    "        weird_pct = 100 * counts.get(\"WEIRD\", 0) / total if total > 0 else 0\n",
    "        rprint(\n",
    "            f\"  {method_name:15s}: WEIRD={counts.get('WEIRD', 0):>3} ({weird_pct:5.1f}%), \"\n",
    "            f\"IN={counts.get('IN', 0):>3}, OUT={counts.get('OUT', 0):>3}, EDGE={counts.get('EDGE', 0):>3}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e0b27",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Critical Insight: WEIRD is Safer Than Misclassification\n",
    "\n",
    "**Misclassifying a segment (e.g., IN‚ÜíOUT) is WORSE than classifying it as WEIRD.**\n",
    "\n",
    "- **Wrong IN/OUT**: Matcher computes similarity with wrong polarity ‚Üí bad matches\n",
    "- **WEIRD fallback**: Triggers flexible matching that doesn't assume shape\n",
    "\n",
    "**Algorithm priority:**\n",
    "\n",
    "1. Minimize false IN/OUT classifications\n",
    "2. Accept some WEIRD as safe fallback\n",
    "3. Only classify IN/OUT when confident\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassification risk: check if mean_y sign matches shape classification\n",
    "# A misclassification is when mean_y suggests one direction but classification says opposite\n",
    "\n",
    "\n",
    "def check_misclassification(stats, method_fn):\n",
    "    \"\"\"Check if classification contradicts the dominant direction.\"\"\"\n",
    "    mean_y = stats[\"mean_y\"]\n",
    "    classification = method_fn(stats)\n",
    "\n",
    "    # Determine \"ground truth\" direction from mean_y\n",
    "    # Positive mean_y ‚Üí segment bulges IN, Negative ‚Üí bulges OUT\n",
    "    CONFIDENCE_TH = 5  # Only consider clear cases\n",
    "\n",
    "    if abs(mean_y) < CONFIDENCE_TH:\n",
    "        # Ambiguous - can't determine ground truth\n",
    "        return \"ambiguous\"\n",
    "\n",
    "    expected = \"IN\" if mean_y > 0 else \"OUT\"\n",
    "\n",
    "    if classification == \"WEIRD\":\n",
    "        return \"weird_safe\"  # WEIRD is always safe\n",
    "    elif classification == \"EDGE\":\n",
    "        if abs(mean_y) > 10:  # Should not be EDGE if clearly displaced\n",
    "            return \"FALSE_EDGE\"  # Classifying IN/OUT as EDGE is BAD\n",
    "        return \"edge_ok\"\n",
    "    elif classification == expected:\n",
    "        return \"correct\"\n",
    "    else:\n",
    "        return \"WRONG_POLARITY\"  # Wrong direction - BAD!\n",
    "\n",
    "\n",
    "rprint(\"\\n[bold red]Misclassification Analysis[/bold red]\")\n",
    "rprint(\"Errors: WRONG_POLARITY = IN‚ÜîOUT swap, FALSE_EDGE = IN/OUT classified as EDGE\")\n",
    "rprint(f\"(Expected max EDGE = {expected_edge}, any excess is likely FALSE_EDGE)\\n\")\n",
    "\n",
    "for method_name, classify_fn in methods.items():\n",
    "    results = [check_misclassification(s, classify_fn) for s in all_stats_v1]\n",
    "    counts = Counter(results)\n",
    "    total = len(results)\n",
    "\n",
    "    # Count classifications\n",
    "    classifications = [classify_fn(s) for s in all_stats_v1]\n",
    "    class_counts = Counter(classifications)\n",
    "    edge_count = class_counts.get(\"EDGE\", 0)\n",
    "    excess_edge = max(0, edge_count - expected_edge)\n",
    "\n",
    "    wrong_polarity = counts.get(\"WRONG_POLARITY\", 0)\n",
    "    false_edge = counts.get(\"FALSE_EDGE\", 0)\n",
    "    total_errors = wrong_polarity + false_edge\n",
    "    weird_safe = counts.get(\"weird_safe\", 0)\n",
    "    correct = counts.get(\"correct\", 0)\n",
    "\n",
    "    # Status based on total errors\n",
    "    if total_errors == 0:\n",
    "        status = \"üü¢ SAFE\"\n",
    "    elif total_errors <= 5:\n",
    "        status = \"üü° OK\"\n",
    "    else:\n",
    "        status = \"üî¥ BAD\"\n",
    "\n",
    "    rprint(\n",
    "        f\"  {method_name:15s}: wrong_pol={wrong_polarity:>3} false_edge={false_edge:>3} | \"\n",
    "        f\"EDGE={edge_count:>3} (excess={excess_edge:>2}) | weird={class_counts.get('WEIRD', 0):>3} | {status}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf2360",
   "metadata": {},
   "source": [
    "## Visualize Classification Boundaries\n",
    "\n",
    "Let's see how each method draws decision boundaries in the mean_y vs signed_area space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how different methods classify segments\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "selected_methods = [\n",
    "    \"Current\",\n",
    "    \"Mean (th=10)\",\n",
    "    \"Mean (th=15)\",\n",
    "    \"Area (th=500)\",\n",
    "    \"Area (th=1000)\",\n",
    "    \"Adaptive\",\n",
    "]\n",
    "class_colors = {\"IN\": \"blue\", \"OUT\": \"green\", \"EDGE\": \"orange\", \"WEIRD\": \"red\"}\n",
    "\n",
    "for ax, method_name in zip(axes, selected_methods):\n",
    "    classify_fn = methods[method_name]\n",
    "\n",
    "    for stats in all_stats_v1:\n",
    "        cls = classify_fn(stats)\n",
    "        color = class_colors.get(cls, \"gray\")\n",
    "        ax.scatter(stats[\"mean_y\"], stats[\"signed_area\"], c=color, alpha=0.5, s=30)\n",
    "\n",
    "    # Add decision boundary lines for reference\n",
    "    ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.axvline(x=0, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    # Count by class\n",
    "    classifications = [classify_fn(s) for s in all_stats_v1]\n",
    "    counts = Counter(classifications)\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{method_name}\\nIN={counts.get('IN', 0)} OUT={counts.get('OUT', 0)} \"\n",
    "        f\"EDGE={counts.get('EDGE', 0)} WEIRD={counts.get('WEIRD', 0)}\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Mean Y\")\n",
    "    ax.set_ylabel(\"Signed Area\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "legend_elements = [Patch(facecolor=c, label=l) for l, c in class_colors.items()]\n",
    "fig.legend(handles=legend_elements, loc=\"upper right\", bbox_to_anchor=(0.99, 0.99))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e20452",
   "metadata": {},
   "source": [
    "## Proposed Safe Classification: Area with WEIRD Fallback\n",
    "\n",
    "A conservative approach that:\n",
    "\n",
    "1. Uses signed area for clear IN/OUT\n",
    "2. Falls back to WEIRD when evidence is weak or contradictory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcd895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_safe_area(stats, area_th=500, edge_th=100):\n",
    "    \"\"\"Safe classification: only classify IN/OUT when confident, else WEIRD.\n",
    "\n",
    "    Args:\n",
    "        area_th: Threshold for confident IN/OUT classification\n",
    "        edge_th: Below this, classify as EDGE (flat segment)\n",
    "    \"\"\"\n",
    "    area = stats[\"signed_area\"]\n",
    "    mean_y = stats[\"mean_y\"]\n",
    "\n",
    "    # Very flat segments are EDGE (strict: both area AND mean must be small)\n",
    "    if abs(area) < edge_th and abs(mean_y) < 5:\n",
    "        return \"EDGE\"\n",
    "\n",
    "    # Check for consistency: area and mean_y should agree on direction\n",
    "    area_says_in = area > area_th\n",
    "    area_says_out = area < -area_th\n",
    "    mean_says_in = mean_y > 10\n",
    "    mean_says_out = mean_y < -10\n",
    "\n",
    "    # Only classify when both metrics agree\n",
    "    if area_says_in and mean_says_in:\n",
    "        return \"IN\"\n",
    "    elif area_says_out and mean_says_out:\n",
    "        return \"OUT\"\n",
    "    elif abs(area) < edge_th and abs(mean_y) < 10:\n",
    "        # Only EDGE if truly flat\n",
    "        return \"EDGE\"\n",
    "    else:\n",
    "        # Ambiguous or contradictory evidence ‚Üí safe WEIRD fallback\n",
    "        return \"WEIRD\"\n",
    "\n",
    "\n",
    "# Test with different thresholds\n",
    "safe_methods = {\n",
    "    \"Safe (500/100)\": lambda s: classify_safe_area(s, area_th=500, edge_th=100),\n",
    "    \"Safe (300/50)\": lambda s: classify_safe_area(s, area_th=300, edge_th=50),\n",
    "    \"Safe (800/150)\": lambda s: classify_safe_area(s, area_th=800, edge_th=150),\n",
    "    \"Safe (1000/200)\": lambda s: classify_safe_area(s, area_th=1000, edge_th=200),\n",
    "}\n",
    "\n",
    "rprint(\"\\n[bold]Safe Classification Methods (WEIRD fallback for ambiguous)[/bold]\")\n",
    "rprint(\n",
    "    f\"Expected: IN={expected_in}, OUT={expected_out}, EDGE‚â§{expected_edge}, WEIRD=0\\n\"\n",
    ")\n",
    "\n",
    "for method_name, classify_fn in safe_methods.items():\n",
    "    classifications = [classify_fn(s) for s in all_stats_v1]\n",
    "    counts = Counter(classifications)\n",
    "\n",
    "    edge_count = counts.get(\"EDGE\", 0)\n",
    "    excess_edge = max(0, edge_count - expected_edge)\n",
    "\n",
    "    # Check misclassification\n",
    "    misclass_results = [check_misclassification(s, classify_fn) for s in all_stats_v1]\n",
    "    misclass_counts = Counter(misclass_results)\n",
    "    wrong_polarity = misclass_counts.get(\"WRONG_POLARITY\", 0)\n",
    "    false_edge = misclass_counts.get(\"FALSE_EDGE\", 0)\n",
    "    total_errors = wrong_polarity + false_edge\n",
    "\n",
    "    status = \"üü¢\" if total_errors == 0 else \"üî¥\"\n",
    "\n",
    "    rprint(\n",
    "        f\"  {method_name:18s}: IN={counts.get('IN', 0):>3} OUT={counts.get('OUT', 0):>3} \"\n",
    "        f\"EDGE={edge_count:>3} (excess={excess_edge:>2}) WEIRD={counts.get('WEIRD', 0):>3} | \"\n",
    "        f\"errors={total_errors} {status}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65ed66",
   "metadata": {},
   "source": [
    "## Summary & Recommendation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c7196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison: Current vs Best Safe method\n",
    "rprint(\"[bold]Final Comparison[/bold]\\n\")\n",
    "\n",
    "current_fn = lambda s: s[\"shape\"].name\n",
    "best_safe_fn = lambda s: classify_safe_area(s, area_th=500, edge_th=100)\n",
    "\n",
    "for label, fn in [\n",
    "    (\"Current Algorithm\", current_fn),\n",
    "    (\"Proposed Safe Area\", best_safe_fn),\n",
    "]:\n",
    "    classifications = [fn(s) for s in all_stats_v1]\n",
    "    counts = Counter(classifications)\n",
    "\n",
    "    edge_count = counts.get(\"EDGE\", 0)\n",
    "    excess_edge = max(0, edge_count - expected_edge)\n",
    "\n",
    "    misclass_results = [check_misclassification(s, fn) for s in all_stats_v1]\n",
    "    misclass_counts = Counter(misclass_results)\n",
    "\n",
    "    wrong_polarity = misclass_counts.get(\"WRONG_POLARITY\", 0)\n",
    "    false_edge = misclass_counts.get(\"FALSE_EDGE\", 0)\n",
    "    total_errors = wrong_polarity + false_edge\n",
    "\n",
    "    rprint(f\"[bold]{label}[/bold]\")\n",
    "    rprint(\n",
    "        f\"  Distribution: IN={counts.get('IN', 0)} OUT={counts.get('OUT', 0)} \"\n",
    "        f\"EDGE={edge_count} (max expected={expected_edge}, excess={excess_edge}) \"\n",
    "        f\"WEIRD={counts.get('WEIRD', 0)}\"\n",
    "    )\n",
    "    rprint(\n",
    "        f\"  Errors: wrong_polarity={wrong_polarity}, false_edge={false_edge}, TOTAL={total_errors}\"\n",
    "    )\n",
    "    rprint(\n",
    "        f\"  Safe (correct + weird): {misclass_counts.get('correct', 0) + misclass_counts.get('weird_safe', 0)}\"\n",
    "    )\n",
    "    rprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a33c9e",
   "metadata": {},
   "source": [
    "## Deep Dive: Adaptive Method Analysis\n",
    "\n",
    "The Adaptive method shows promising results:\n",
    "\n",
    "- IN=78 (expected 82), OUT=71 (expected 82), EDGE=27 (expected 28)\n",
    "- Only 16 WEIRD (~8%) - much better than current 110\n",
    "\n",
    "Let's investigate:\n",
    "\n",
    "1. What are the 16 WEIRD segments? Are they truly ambiguous?\n",
    "2. Are there any misclassifications hiding?\n",
    "3. How do the adaptive thresholds behave?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9208572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of Adaptive method\n",
    "def classify_adaptive_detailed(stats):\n",
    "    \"\"\"Adaptive classification with detailed breakdown.\"\"\"\n",
    "    y_vals = stats[\"y_vals\"]\n",
    "    flat_th = max(10, np.std(y_vals) * 1.5)\n",
    "    count_th = max(3, len(y_vals) * 0.05)\n",
    "\n",
    "    out_count = (y_vals < -flat_th).sum()\n",
    "    in_count = (y_vals > flat_th).sum()\n",
    "\n",
    "    is_out = bool(out_count > count_th)\n",
    "    is_in = bool(in_count > count_th)\n",
    "\n",
    "    match (is_out, is_in):\n",
    "        case (True, False):\n",
    "            result = \"OUT\"\n",
    "        case (False, True):\n",
    "            result = \"IN\"\n",
    "        case (False, False):\n",
    "            result = \"EDGE\"\n",
    "        case (True, True):\n",
    "            result = \"WEIRD\"\n",
    "\n",
    "    return {\n",
    "        \"classification\": result,\n",
    "        \"flat_th\": flat_th,\n",
    "        \"count_th\": count_th,\n",
    "        \"out_count\": out_count,\n",
    "        \"in_count\": in_count,\n",
    "        \"is_out\": is_out,\n",
    "        \"is_in\": is_in,\n",
    "        \"std_y\": np.std(y_vals),\n",
    "        \"n_points\": len(y_vals),\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze all segments with adaptive method\n",
    "adaptive_results = []\n",
    "for stats in all_stats_v1:\n",
    "    result = classify_adaptive_detailed(stats)\n",
    "    result[\"mean_y\"] = stats[\"mean_y\"]\n",
    "    result[\"signed_area\"] = stats[\"signed_area\"]\n",
    "    result[\"piece_id\"] = stats[\"piece_id\"]\n",
    "    result[\"seg_id\"] = stats[\"seg_id\"]\n",
    "    result[\"original_shape\"] = stats[\"shape\"].name\n",
    "    adaptive_results.append(result)\n",
    "\n",
    "# Summary\n",
    "classifications = Counter(r[\"classification\"] for r in adaptive_results)\n",
    "rprint(\"[bold]Adaptive Method Summary[/bold]\")\n",
    "rprint(f\"  IN:    {classifications['IN']:>3} (expected {expected_in})\")\n",
    "rprint(f\"  OUT:   {classifications['OUT']:>3} (expected {expected_out})\")\n",
    "rprint(f\"  EDGE:  {classifications['EDGE']:>3} (expected {expected_edge})\")\n",
    "rprint(f\"  WEIRD: {classifications['WEIRD']:>3} (expected 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f21479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the 16 WEIRD segments from Adaptive method\n",
    "weird_adaptive = [r for r in adaptive_results if r[\"classification\"] == \"WEIRD\"]\n",
    "\n",
    "rprint(f\"\\n[bold]The {len(weird_adaptive)} WEIRD segments from Adaptive[/bold]\")\n",
    "rprint(\n",
    "    \"These have points on BOTH sides of the center line (is_out AND is_in both True)\\n\"\n",
    ")\n",
    "\n",
    "for i, w in enumerate(weird_adaptive):\n",
    "    rprint(f\"[{i + 1:>2}] Piece {w['piece_id']}, Seg {w['seg_id']}\")\n",
    "    rprint(f\"     mean_y={w['mean_y']:>7.1f}, area={w['signed_area']:>8.1f}\")\n",
    "    rprint(f\"     flat_th={w['flat_th']:>5.1f}, count_th={w['count_th']:>4.1f}\")\n",
    "    rprint(f\"     out_count={w['out_count']:>3}, in_count={w['in_count']:>3}\")\n",
    "    rprint(f\"     Original: {w['original_shape']}\")\n",
    "    rprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401dab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for errors in Adaptive: compare mean_y direction with classification\n",
    "rprint(\"[bold]Adaptive Error Analysis[/bold]\")\n",
    "rprint(\"Checking if adaptive classifications match the dominant direction (mean_y)\\n\")\n",
    "\n",
    "errors = {\n",
    "    \"wrong_polarity\": [],\n",
    "    \"false_edge\": [],\n",
    "    \"correct\": [],\n",
    "    \"weird_safe\": [],\n",
    "    \"ambiguous\": [],\n",
    "}\n",
    "\n",
    "for r in adaptive_results:\n",
    "    mean_y = r[\"mean_y\"]\n",
    "    cls = r[\"classification\"]\n",
    "\n",
    "    # Determine expected based on mean_y\n",
    "    if abs(mean_y) < 5:\n",
    "        errors[\"ambiguous\"].append(r)\n",
    "        continue\n",
    "\n",
    "    expected = \"IN\" if mean_y > 0 else \"OUT\"\n",
    "\n",
    "    if cls == \"WEIRD\":\n",
    "        errors[\"weird_safe\"].append(r)\n",
    "    elif cls == \"EDGE\":\n",
    "        if abs(mean_y) > 10:\n",
    "            errors[\"false_edge\"].append(r)\n",
    "        else:\n",
    "            errors[\"ambiguous\"].append(r)\n",
    "    elif cls == expected:\n",
    "        errors[\"correct\"].append(r)\n",
    "    else:\n",
    "        errors[\"wrong_polarity\"].append(r)\n",
    "\n",
    "rprint(f\"  Correct IN/OUT:    {len(errors['correct']):>3}\")\n",
    "rprint(f\"  WEIRD (safe):      {len(errors['weird_safe']):>3}\")\n",
    "rprint(f\"  Ambiguous:         {len(errors['ambiguous']):>3}\")\n",
    "rprint(f\"  [red]Wrong polarity:  {len(errors['wrong_polarity']):>3}[/red]\")\n",
    "rprint(f\"  [red]False EDGE:      {len(errors['false_edge']):>3}[/red]\")\n",
    "\n",
    "total_errors = len(errors[\"wrong_polarity\"]) + len(errors[\"false_edge\"])\n",
    "rprint(f\"\\n  [bold]TOTAL ERRORS: {total_errors}[/bold]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf2c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are wrong polarity errors, show them\n",
    "if errors[\"wrong_polarity\"]:\n",
    "    rprint(\n",
    "        \"\\n[bold red]WRONG POLARITY ERRORS (Adaptive classified opposite direction)[/bold red]\"\n",
    "    )\n",
    "    for r in errors[\"wrong_polarity\"]:\n",
    "        expected = \"IN\" if r[\"mean_y\"] > 0 else \"OUT\"\n",
    "        rprint(\n",
    "            f\"  Piece {r['piece_id']}, Seg {r['seg_id']}: classified {r['classification']}, expected {expected}\"\n",
    "        )\n",
    "        rprint(f\"    mean_y={r['mean_y']:.1f}, area={r['signed_area']:.1f}\")\n",
    "        rprint(f\"    out_count={r['out_count']}, in_count={r['in_count']}\")\n",
    "else:\n",
    "    rprint(\"\\n[green]‚úì No wrong polarity errors![/green]\")\n",
    "\n",
    "if errors[\"false_edge\"]:\n",
    "    rprint(\n",
    "        \"\\n[bold red]FALSE EDGE ERRORS (Adaptive classified as EDGE but clearly IN/OUT)[/bold red]\"\n",
    "    )\n",
    "    for r in errors[\"false_edge\"]:\n",
    "        expected = \"IN\" if r[\"mean_y\"] > 0 else \"OUT\"\n",
    "        rprint(\n",
    "            f\"  Piece {r['piece_id']}, Seg {r['seg_id']}: classified EDGE, expected {expected}\"\n",
    "        )\n",
    "        rprint(f\"    mean_y={r['mean_y']:.1f}, area={r['signed_area']:.1f}\")\n",
    "else:\n",
    "    rprint(\"\\n[green]‚úì No false EDGE errors![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the WEIRD segments from Adaptive method\n",
    "# These are the \"truly ambiguous\" ones that have points on both sides\n",
    "\n",
    "if weird_adaptive:\n",
    "    n_to_show = min(6, len(weird_adaptive))\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Get the actual segment objects for these WEIRD ones\n",
    "    weird_segs_adaptive = []\n",
    "    for w in weird_adaptive[:n_to_show]:\n",
    "        for piece in manager_v1.get_pieces_ls():\n",
    "            if piece.piece_id == w[\"piece_id\"]:\n",
    "                for seg_id, seg in piece.segments.items():\n",
    "                    if seg_id == w[\"seg_id\"]:\n",
    "                        weird_segs_adaptive.append((w, seg))\n",
    "                        break\n",
    "\n",
    "    for i, (w, seg) in enumerate(weird_segs_adaptive[:6]):\n",
    "        ax = axes[i]\n",
    "        pts = seg.points.squeeze()\n",
    "\n",
    "        ax.plot(pts[:, 0], pts[:, 1], \"b-\", linewidth=2)\n",
    "        ax.scatter(pts[0, 0], pts[0, 1], c=\"green\", s=100, zorder=5, label=\"Start\")\n",
    "        ax.scatter(pts[-1, 0], pts[-1, 1], c=\"red\", s=100, zorder=5, label=\"End\")\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"Piece {w['piece_id']},\\nSeg {w['seg_id']}\\n\"\n",
    "            f\"mean_y={w['mean_y']:.1f}, area={w['signed_area']:.0f}\\n\"\n",
    "            f\"out={w['out_count']}, in={w['in_count']} (th={w['count_th']:.1f})\"\n",
    "        )\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(len(weird_segs_adaptive), 6):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.suptitle(\"Segments classified as WEIRD by Adaptive method\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of adaptive thresholds\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# flat_th distribution\n",
    "flat_ths = [r[\"flat_th\"] for r in adaptive_results]\n",
    "axes[0].hist(flat_ths, bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].axvline(x=20, color=\"red\", linestyle=\"--\", label=\"Current fixed (20)\")\n",
    "axes[0].set_xlabel(\"flat_th (adaptive)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\n",
    "    f\"flat_th distribution\\nmin={min(flat_ths):.1f}, max={max(flat_ths):.1f}, mean={np.mean(flat_ths):.1f}\"\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "# count_th distribution\n",
    "count_ths = [r[\"count_th\"] for r in adaptive_results]\n",
    "axes[1].hist(count_ths, bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].axvline(x=5, color=\"red\", linestyle=\"--\", label=\"Current fixed (5)\")\n",
    "axes[1].set_xlabel(\"count_th (adaptive)\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\n",
    "    f\"count_th distribution\\nmin={min(count_ths):.1f}, max={max(count_ths):.1f}, mean={np.mean(count_ths):.1f}\"\n",
    ")\n",
    "axes[1].legend()\n",
    "\n",
    "# std_y distribution (drives flat_th)\n",
    "std_ys = [r[\"std_y\"] for r in adaptive_results]\n",
    "axes[2].hist(std_ys, bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "axes[2].set_xlabel(\"std(y_vals)\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_title(\n",
    "    f\"Standard deviation of y values\\nmin={min(std_ys):.1f}, max={max(std_ys):.1f}\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "rprint(f\"\\n[bold]Adaptive threshold statistics:[/bold]\")\n",
    "rprint(\n",
    "    f\"  flat_th:  min={min(flat_ths):.1f}, max={max(flat_ths):.1f}, mean={np.mean(flat_ths):.1f} (fixed=20)\"\n",
    ")\n",
    "rprint(\n",
    "    f\"  count_th: min={min(count_ths):.1f}, max={max(count_ths):.1f}, mean={np.mean(count_ths):.1f} (fixed=5)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cfd517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Current vs Adaptive on the same scatter plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, (method_name, method_fn) in zip(\n",
    "    axes,\n",
    "    [\n",
    "        (\"Current Algorithm\", lambda s: s[\"shape\"].name),\n",
    "        (\"Adaptive Method\", classify_adaptive),\n",
    "    ],\n",
    "):\n",
    "    for stats in all_stats_v1:\n",
    "        cls = method_fn(stats)\n",
    "        color = class_colors.get(cls, \"gray\")\n",
    "        ax.scatter(stats[\"mean_y\"], stats[\"signed_area\"], c=color, alpha=0.5, s=40)\n",
    "\n",
    "    ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.axvline(x=0, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    classifications = [method_fn(s) for s in all_stats_v1]\n",
    "    counts = Counter(classifications)\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{method_name}\\nIN={counts.get('IN', 0)} OUT={counts.get('OUT', 0)} \"\n",
    "        f\"EDGE={counts.get('EDGE', 0)} WEIRD={counts.get('WEIRD', 0)}\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Mean Y (positive=IN, negative=OUT)\")\n",
    "    ax.set_ylabel(\"Signed Area\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "legend_elements = [Patch(facecolor=c, label=l) for l, c in class_colors.items()]\n",
    "fig.legend(handles=legend_elements, loc=\"upper right\", bbox_to_anchor=(0.99, 0.99))\n",
    "\n",
    "plt.suptitle(\"Current vs Adaptive Classification\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415488a",
   "metadata": {},
   "source": [
    "## Adaptive Method Verdict\n",
    "\n",
    "Based on the analysis above:\n",
    "\n",
    "- **Errors**: How many wrong polarity / false EDGE?\n",
    "- **WEIRD segments**: Are they genuinely ambiguous (points on both sides)?\n",
    "- **Threshold behavior**: Do adaptive thresholds make sense?\n",
    "\n",
    "If Adaptive has 0 errors and only classifies genuinely ambiguous segments as WEIRD, it may be the best choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e78086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verdict on Adaptive method\n",
    "rprint(\"[bold]ADAPTIVE METHOD FINAL VERDICT[/bold]\\n\")\n",
    "\n",
    "# Count errors\n",
    "adaptive_wrong_pol = len(errors[\"wrong_polarity\"])\n",
    "adaptive_false_edge = len(errors[\"false_edge\"])\n",
    "adaptive_total_errors = adaptive_wrong_pol + adaptive_false_edge\n",
    "\n",
    "# Get classifications\n",
    "adaptive_cls = Counter(r[\"classification\"] for r in adaptive_results)\n",
    "\n",
    "rprint(\"[bold]Classification counts:[/bold]\")\n",
    "rprint(\n",
    "    f\"  IN:    {adaptive_cls['IN']:>3} / {expected_in} expected (diff: {adaptive_cls['IN'] - expected_in:+d})\"\n",
    ")\n",
    "rprint(\n",
    "    f\"  OUT:   {adaptive_cls['OUT']:>3} / {expected_out} expected (diff: {adaptive_cls['OUT'] - expected_out:+d})\"\n",
    ")\n",
    "rprint(\n",
    "    f\"  EDGE:  {adaptive_cls['EDGE']:>3} / {expected_edge} expected (diff: {adaptive_cls['EDGE'] - expected_edge:+d})\"\n",
    ")\n",
    "rprint(f\"  WEIRD: {adaptive_cls['WEIRD']:>3} / 0 expected\")\n",
    "\n",
    "rprint(f\"\\n[bold]Error analysis:[/bold]\")\n",
    "rprint(f\"  Wrong polarity (IN‚ÜîOUT): {adaptive_wrong_pol}\")\n",
    "rprint(f\"  False EDGE (IN/OUT‚ÜíEDGE): {adaptive_false_edge}\")\n",
    "rprint(f\"  [bold]TOTAL ERRORS: {adaptive_total_errors}[/bold]\")\n",
    "\n",
    "if adaptive_total_errors == 0:\n",
    "    rprint(\"\\n[bold green]‚úì ADAPTIVE IS SAFE - No misclassifications![/bold green]\")\n",
    "    rprint(\n",
    "        f\"  The {adaptive_cls['WEIRD']} WEIRD classifications are genuinely ambiguous segments.\"\n",
    "    )\n",
    "    rprint(\"  This is the BEST outcome: minimize errors, accept some safe WEIRD.\")\n",
    "else:\n",
    "    rprint(f\"\\n[bold red]‚ö† ADAPTIVE HAS {adaptive_total_errors} ERRORS[/bold red]\")\n",
    "\n",
    "# Compare to current\n",
    "current_cls = Counter(s[\"shape\"].name for s in all_stats_v1)\n",
    "rprint(f\"\\n[bold]Comparison to Current algorithm:[/bold]\")\n",
    "rprint(f\"  Current WEIRD:  {current_cls['WEIRD']:>3} (57%)\")\n",
    "rprint(\n",
    "    f\"  Adaptive WEIRD: {adaptive_cls['WEIRD']:>3} ({100 * adaptive_cls['WEIRD'] / len(all_stats_v1):.0f}%)\"\n",
    ")\n",
    "rprint(\n",
    "    f\"  Reduction: {current_cls['WEIRD'] - adaptive_cls['WEIRD']} fewer WEIRD classifications\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snap-fit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
